The project `happy-phone-llm` is a self contained mobile LLM framework based on flutter which include kernels for LLM inference on ARM chips, a inference engine that supports LLM inference, speculative decoding, etc, script for training, finetuning model, script for converting model to GGUF format, and flutter bindings for the kernel and inference engine. The flutter library for the binding is created under `happy_phone_llm_flutter`. The code for model training, model finetuning, and model format conversion should be implemented in python inside `model` directory. The model inference code should be implemented using cpp inside `inference` directory, and the kernels should be implemented using cpp inside `inference/kernels` directory. An example app is created in `example_app` directory. It should be used for testing the flutter binding and also the llm inference. Use https://github.com/cactus-compute/cactus-flutter as an example for LLM flutter binding implementation. Use https://github.com/cactus-compute/cactus as an example for mobile LLM inference engine and kernel implementation. Use https://github.com/ggml-org/llama.cpp as an example for generic portable cross-platform LLM inference engine and kernel implementation. Conduct a research on how to build this framework, and write a detailed report with functionalities, technical stack choices, roadmap, effort estimate, todo tasks inside `spec` with name `research-001.md`.